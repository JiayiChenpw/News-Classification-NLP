{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a8FlKyBLKL7k"
   },
   "source": [
    "# News Classification NLP Project\n",
    "Feng Jiang, Jiayi Chen, Zihan Wang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xiGEPdPXAtFW",
    "outputId": "71218d8c-f8da-4da2-b9cf-7b973a4a1f4e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "gcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# Installing dependencies\n",
    "!pip install geopy > delete.txt\n",
    "!pip install datasets > delete.txt\n",
    "!pip install torch torchvision datasets > delete.txt\n",
    "!pip install huggingface_hub > delete.txt\n",
    "!rm delete.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SxB4dXxpAyJ7",
    "outputId": "cda8199c-5260-4c7a-8878-3f091f7fb312"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n",
      "    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n",
      "    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n",
      "    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n",
      "    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n",
      "\n",
      "    To log in, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .\n",
      "Enter your token (input will not be visible): \n",
      "Add token as git credential? (Y/n) n\n",
      "Token is valid (permission: fineGrained).\n",
      "The token `5190news` has been saved to /root/.cache/huggingface/stored_tokens\n",
      "Your token has been saved to /root/.cache/huggingface/token\n",
      "Login successful.\n",
      "The current active token is: `5190news`\n"
     ]
    }
   ],
   "source": [
    "# Huggingface login\n",
    "!huggingface-cli login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zZFAPcZQhqdU"
   },
   "outputs": [],
   "source": [
    "#Imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import re\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import LinearSVC\n",
    "import tensorflow as tf\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import PreTrainedModel, PretrainedConfig\n",
    "from transformers.utils import logging\n",
    "from typing import Dict, List, Optional, Union\n",
    "from transformers.modeling_outputs import SequenceClassifierOutput\n",
    "from huggingface_hub import push_to_hub_keras\n",
    "from huggingface_hub import notebook_login\n",
    "from huggingface_hub import login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "k7bs8pPEhsz9",
    "outputId": "b3e5bb45-1469-4414-b1f4-905bb7f3131f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZvbsidgKhPrw"
   },
   "source": [
    "# Class 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "47L1CAQ8JBVy"
   },
   "outputs": [],
   "source": [
    "class NewsClassifierConfig(PretrainedConfig):\n",
    "    model_type = \"news_classifier\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_labels: int = 2,\n",
    "        embedding_dim: int = 100,\n",
    "        **kwargs\n",
    "    ):\n",
    "        self.num_labels = num_labels\n",
    "        self.embedding_dim = embedding_dim\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "class NewsClassifier(PreTrainedModel):\n",
    "    config_class = NewsClassifierConfig\n",
    "\n",
    "    def __init__(self, config: NewsClassifierConfig):\n",
    "        super().__init__(config)\n",
    "        self.num_labels = config.num_labels\n",
    "        self.glove_embeddings = {}\n",
    "        self.svm_classifier = LinearSVC()\n",
    "        self.embedding_dim = config.embedding_dim\n",
    "\n",
    "        # Initialize a simple linear layer for PyTorch compatibility\n",
    "        self.classifier = nn.Linear(self.embedding_dim, self.num_labels)\n",
    "\n",
    "    def load_glove(self, glove_path: str):\n",
    "        \"\"\"Load GloVe embeddings from file.\"\"\"\n",
    "        print(\"Loading GloVe embeddings...\")\n",
    "        with open(glove_path, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                values = line.split()\n",
    "                word = values[0]\n",
    "                vector = np.asarray(values[1:], dtype='float32')\n",
    "                self.glove_embeddings[word] = vector\n",
    "        print(\"GloVe embeddings loaded.\")\n",
    "\n",
    "    def clean_text(self, text: str) -> str:\n",
    "        \"\"\"Clean and preprocess text.\"\"\"\n",
    "        text = str(text)\n",
    "        contractions = {\n",
    "            \"n't\": \" not\",\n",
    "            \"'s\": \" is\",\n",
    "            \"'ll\": \" will\",\n",
    "            \"'ve\": \" have\"\n",
    "        }\n",
    "        for contraction, expansion in contractions.items():\n",
    "            text = text.replace(contraction, expansion)\n",
    "        text = re.sub(r'\\$(\\\\d+)\\\\.?\\\\d*\\\\s*(million|billion|trillion)?', r'$ \\\\1', text, flags=re.IGNORECASE)\n",
    "        text = re.sub(r'http\\\\S+', '', text)\n",
    "        text = re.sub(r'-', ' ', text)\n",
    "        text = text.lower()\n",
    "        text = ' '.join(text.split())\n",
    "        return text\n",
    "\n",
    "    def get_document_vector(self, text: str) -> np.ndarray:\n",
    "        \"\"\"Convert text to document vector using GloVe embeddings.\"\"\"\n",
    "        words = text.split()\n",
    "        words = [word for word in words if word in self.glove_embeddings]\n",
    "        if not words:\n",
    "            return np.zeros(self.embedding_dim)\n",
    "        vectors = [self.glove_embeddings[word] for word in words]\n",
    "        return np.mean(vectors, axis=0)\n",
    "\n",
    "    def fit(self, texts: List[str], labels: List[int]):\n",
    "        \"\"\"Train the SVM classifier on the embedded texts.\"\"\"\n",
    "        # Preprocess texts and create document vectors\n",
    "        X = np.stack([self.get_document_vector(self.clean_text(text)) for text in texts])\n",
    "        self.svm_classifier.fit(X, labels)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_text: Union[str, List[str]] = None,\n",
    "        labels: Optional[torch.LongTensor] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "    ) -> SequenceClassifierOutput:\n",
    "        \"\"\"Forward pass for PyTorch compatibility.\"\"\"\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        if isinstance(input_text, list):\n",
    "            # Process batch of texts\n",
    "            embeddings = torch.tensor(\n",
    "                np.stack([self.get_document_vector(self.clean_text(text)) for text in input_text])\n",
    "            ).float()\n",
    "        else:\n",
    "            # Process single text\n",
    "            embeddings = torch.tensor(\n",
    "                self.get_document_vector(self.clean_text(input_text))\n",
    "            ).float().unsqueeze(0)\n",
    "\n",
    "        logits = self.classifier(embeddings)\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss_fct = nn.CrossEntropyLoss()\n",
    "            loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "\n",
    "        if not return_dict:\n",
    "            output = (logits,)\n",
    "            return ((loss,) + output) if loss is not None else output\n",
    "\n",
    "        return SequenceClassifierOutput(\n",
    "            loss=loss,\n",
    "            logits=logits\n",
    "        )\n",
    "\n",
    "    def predict(self, texts: List[str]) -> np.ndarray:\n",
    "        \"\"\"Predict labels for given texts.\"\"\"\n",
    "        X = np.stack([self.get_document_vector(self.clean_text(text)) for text in texts])\n",
    "        return self.svm_classifier.predict(X)\n",
    "\n",
    "    def save_pretrained(self, save_directory: str, **kwargs):\n",
    "        \"\"\"Save the model and its configuration.\"\"\"\n",
    "        super().save_pretrained(save_directory, **kwargs)\n",
    "\n",
    "        import os\n",
    "        import pickle\n",
    "\n",
    "        # Save SVM classifier\n",
    "        with open(os.path.join(save_directory, 'svm_classifier.pkl'), 'wb') as f:\n",
    "            pickle.dump(self.svm_classifier, f)\n",
    "\n",
    "        # Save GloVe embeddings\n",
    "        with open(os.path.join(save_directory, 'glove_embeddings.pkl'), 'wb') as f:\n",
    "            pickle.dump(self.glove_embeddings, f)\n",
    "\n",
    "    @classmethod\n",
    "    def from_pretrained(cls, pretrained_model_name_or_path: str, *model_args, **kwargs):\n",
    "        \"\"\"Load a pretrained model.\"\"\"\n",
    "        model = super().from_pretrained(pretrained_model_name_or_path, *model_args, **kwargs)\n",
    "\n",
    "        import os\n",
    "        import pickle\n",
    "\n",
    "        # Load SVM classifier if exists\n",
    "        svm_path = os.path.join(pretrained_model_name_or_path, 'svm_classifier.pkl')\n",
    "        if os.path.exists(svm_path):\n",
    "            with open(svm_path, 'rb') as f:\n",
    "                model.svm_classifier = pickle.load(f)\n",
    "\n",
    "        # Load GloVe embeddings if exist\n",
    "        glove_path = os.path.join(pretrained_model_name_or_path, 'glove_embeddings.pkl')\n",
    "        if os.path.exists(glove_path):\n",
    "            with open(glove_path, 'rb') as f:\n",
    "                model.glove_embeddings = pickle.load(f)\n",
    "\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "F9FIugo3K8yk",
    "outputId": "8e910e24-bf9d-4837-e55b-bc1d61cf52c7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-12-14 04:28:34--  http://nlp.stanford.edu/data/glove.6B.zip\n",
      "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
      "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
      "--2024-12-14 04:28:34--  https://nlp.stanford.edu/data/glove.6B.zip\n",
      "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
      "HTTP request sent, awaiting response... 301 Moved Permanently\n",
      "Location: https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
      "--2024-12-14 04:28:35--  https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
      "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
      "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 862182613 (822M) [application/zip]\n",
      "Saving to: ‘glove.6B.zip’\n",
      "\n",
      "glove.6B.zip        100%[===================>] 822.24M  2.23MB/s    in 5m 35s  \n",
      "\n",
      "2024-12-14 04:34:11 (2.45 MB/s) - ‘glove.6B.zip’ saved [862182613/862182613]\n",
      "\n",
      "Archive:  /content/glove.6B.zip\n",
      "replace /content/drive/My Drive/5190_project/glove.6B.100d.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
      "  inflating: /content/drive/My Drive/5190_project/glove.6B.100d.txt  \n",
      "'Copy of CIS5190_Final_Project.ipynb'   glove.6B.200d.txt\t  url_only_data.gsheet\n",
      " glove.6B.100d.txt\t\t        scraped_news_data_1.csv\n"
     ]
    }
   ],
   "source": [
    "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
    "!unzip /content/glove.6B.zip glove.6B.100d.txt -d /content/drive/My\\ Drive/5190_project/\n",
    "!ls \"/content/drive/My Drive/5190_project/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 181,
     "referenced_widgets": [
      "c34453da3747405993785c4f1af2a568",
      "9ec330c6cac24d34b215d9073bac2352",
      "ffd8a75200244307b06ff285cf9c12c8",
      "d166bf03008749e3b9286ff001dd42fa",
      "cc06e3bfeeb54efe8e4e7524f3e52cdd",
      "6587e6eead6f41c2a8a605a995aabefc",
      "c7d549b60d5e4bf8a2bd3f4c370fa2ae",
      "e0017a8ceb254749beff7e13f5b5c50c",
      "46d485f73ff94af99bcec6ae00a4e9de",
      "1431f48e813c41d99db87baeeae76145",
      "ed1903a3be93431c946edbc1bf3d6649",
      "6f1ff5653d054ceea7b18a637f8b2bde",
      "fac60b1fd82c49d998a41c0345acf297",
      "70a528cd824a484497303fce5c0924eb",
      "734888b3cc994929a9a02cb504e5eaf9",
      "4afc7a73545649639cc64fae567c33ce",
      "6dac29b4071141caa6147eabac256d5e",
      "0eeaac9c0ccf4ad6ba11d2f802952dba",
      "d2503c80769346ab8ec7671858d5404d",
      "ba38217efebc478a8de67f047c88856f",
      "451699631e6d4922852a16c7351045ad",
      "dc820562b818415b81dd991c61dc7e86",
      "8b305013a95541aba5bc525aeb0581de",
      "70059ae0f2544283b7ffec5752fc873f",
      "df63d9de81ab48d197f0a649fd0dd218",
      "dcd71c4974ae4fe5b44bdc31283dea9d",
      "2ad0ed8b94254eb8bbfa9f92bb48d962",
      "8c0f7c4bbe2e40f2828fa6defca997b8",
      "85479d6472d0486a91598428c863f321",
      "0638a2abc41649839e616f2411bcbf32",
      "53200bfa67694e11b5522423547f0477",
      "571bc132808c45a9b66936c8414935ca",
      "98771761dd3c495c8b65c4ecc2e137b1",
      "2ca94fdfe89b4d2c968635e5e9ab05f7",
      "07a4ce2b8f06435a914f990afd066bb0",
      "3d40810ed433408488f9097d3ced2dfc",
      "f5a13d42263e4839934a966f5b299a1b",
      "cb0464fa0d1a4f1a9beaca61a5aafdcf",
      "244e406d406b409cb22a2ccf9c9225dd",
      "aa1a740a4b5d479da1c3bc92c5990792",
      "5fe372a9f8df4e578f04a0ae44dbc84f",
      "d5f02a544fc7469f8d8643c15e125a34",
      "e383b6962179408d9b92f8a9769535aa",
      "1c49cfc5fec3469890f2842f949e34cf"
     ]
    },
    "id": "lFCRfk1jmgT2",
    "outputId": "c8aed420-4ea5-43bf-bc5e-3b2e40d57b13"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pushing model to Hugging Face Hub (CIS5190GoGo/NewsClassifierConfig)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c34453da3747405993785c4f1af2a568",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/992 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f1ff5653d054ceea7b18a637f8b2bde",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "svm_classifier.pkl:   0%|          | 0.00/1.40k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b305013a95541aba5bc525aeb0581de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "glove_embeddings.pkl:   0%|          | 0.00/177M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ca94fdfe89b4d2c968635e5e9ab05f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload 3 LFS files:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model successfully pushed to https://huggingface.co/CIS5190GoGo/NewsClassifierConfig\n"
     ]
    }
   ],
   "source": [
    "# Push to Hugging Face\n",
    "REPO_NAME = \"CIS5190GoGo/NewsClassifierConfig\"\n",
    "print(f\"Pushing model to Hugging Face Hub ({REPO_NAME})...\")\n",
    "model.push_to_hub(REPO_NAME)\n",
    "print(f\"Model successfully pushed to https://huggingface.co/{REPO_NAME}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JRwJbz-Ks1-D"
   },
   "source": [
    "# Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vnGtXZMG96lJ"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset, TensorDataset\n",
    "from transformers import DistilBertForSequenceClassification, DistilBertTokenizer, AdamW, get_scheduler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H6zEuNiWutHk"
   },
   "outputs": [],
   "source": [
    "news_data = pd.read_csv(\"/content/drive/My Drive/5190_project/news_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aEtfk1vd9zRv"
   },
   "outputs": [],
   "source": [
    "# Ensure GPU is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load data\n",
    "X = news_data[\"title\"].values\n",
    "y = news_data[\"labels\"].values\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 853,
     "referenced_widgets": [
      "91dfb8498b2346f88e5bcc5833640ec1",
      "eeb1f3cd86f04861856be45c5876e7c8",
      "bbe71fd991e34bdd9a7dfaf333ff174c",
      "8af1cb0d703743f0b1c8b630b29ead99",
      "14a552b0f31749cab40292adc06adb2e",
      "8a1783baac204c73b5211d2b7db18917",
      "fb69e61a659d4bf8ba83559674da8d6e",
      "f7122aef0dae4cb3bdd2deeb2618b078",
      "37adef62f52249a1bf8976464083482f",
      "a16032a24c99442dac93f2aa1137fe9e",
      "a002dbce770f467e9b52e95274266006",
      "449de69023ed4a7380c6325f3c71da1d",
      "0ec1547e19a94857b068a74fbfafb25c",
      "ab599460a8a846f1b991988f573b71ed",
      "d135a4664598471f914106f22c0ab237",
      "55944d0dbf034f158b1e03b519a55008",
      "9cdc90ff0eae485d82585d116e662688",
      "7178a34439b141f092b0cc384cb01b69",
      "ad294c0f7da445869ece8eec2bf1918b",
      "cf24c95040f84682929250a5667e5e95",
      "d7d450c5c16a4e6f8fdc43ec7cba0ea4",
      "f3b0cb94ce8f43b1acd0dc01d39fea03",
      "35828ee9f19749cf8030d5316ff4082e",
      "97e6ca94bfe140c28a1982cfae93c16f",
      "f2b56a44db1e483f9df1f8bf7828d515",
      "99b0f56520454ff89dbbba9bf1b03488",
      "8691c7e248ba492d9285b7a6549f9b88",
      "c013fb047f6e4e49a44df1a3d8b16e08",
      "54d7269e50114ac48e4257ec874567fe",
      "3e28c4d989044d79a270e83cdc5285be",
      "9f50bba2c80648f1a3ee51bd363e7b0b",
      "bf5c9111e40548b7aec8a01eb3e8b634",
      "e180f7f7a1ca4de7b65072a3d58c60b4",
      "c36dff6a7d0545c9b353d5ea0248e227",
      "2046a87e29b04a54beb8d7c753547fbc",
      "619befcb22b84494952eba42410ea7ac",
      "67ffa4db326c4ddf9f10f651be66709c",
      "2c75a224eaf64a23b0607086bd0358e0",
      "e80d9ee227f5463ca6521da99b2b7d31",
      "ed4047445a804c738a6e2fa23be9498c",
      "8eb40a890fb1491681448cb51acfd1d0",
      "be3b4693db184e61a204c24dd14f1fe3",
      "91f84023d14f4ed18f668ae38fc1fb26",
      "a4a68a3a62d34ef5b829f8533d645397",
      "1026844c88a5418e8ec207282a9dbf14",
      "f11b4ad34cdc424fa4f2e991128f058e",
      "e8f24a365bb7482f98d8c14793013343",
      "8e0e8d78a6a6402995ee6c6d23f824a7",
      "43da0675c2d84ae7883e8dee3e1f9c73",
      "73c70f50d55543829f0dba0842ae7374",
      "d4b683cb74d64a7daec7d5718c8527f2",
      "248cc5d78d344903bbc8746347de873a",
      "3047aacb988e49e2b87374f2eee9f0d2",
      "4f038ba6a43e4482a2003148929783b7",
      "5e793a78fb4944c5af3e900fd6a8ce48"
     ]
    },
    "id": "yqQI1zuc9tXA",
    "outputId": "9fd8dba3-ef6e-42a4-ac2d-97daa1f50e4b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing tokenizer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91dfb8498b2346f88e5bcc5833640ec1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "449de69023ed4a7380c6325f3c71da1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35828ee9f19749cf8030d5316ff4082e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c36dff6a7d0545c9b353d5ea0248e227",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing datasets...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1026844c88a5418e8ec207282a9dbf14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Step 0, Loss: 0.7017\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-339b8fa42ae8>\u001b[0m in \u001b[0;36m<cell line: 41>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0mlr_scheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    579\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    580\u001b[0m             )\n\u001b[0;32m--> 581\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    582\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    583\u001b[0m         )\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    345\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    348\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    823\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    824\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 825\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    826\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Tokenization\n",
    "print(\"Initializing tokenizer...\")\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "def preprocess_data(texts, labels, tokenizer, max_len=128):\n",
    "    \"\"\"Preprocess the data: tokenize and prepare tensors.\"\"\"\n",
    "    encodings = tokenizer(\n",
    "        texts.tolist(),\n",
    "        max_length=max_len,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    return encodings[\"input_ids\"], encodings[\"attention_mask\"], torch.tensor(labels)\n",
    "\n",
    "# Preprocess the datasets\n",
    "print(\"Tokenizing datasets...\")\n",
    "train_inputs, train_masks, train_labels = preprocess_data(X_train, y_train, tokenizer)\n",
    "test_inputs, test_masks, test_labels = preprocess_data(X_test, y_test, tokenizer)\n",
    "\n",
    "# Create DataLoader objects\n",
    "train_dataset = TensorDataset(train_inputs, train_masks, train_labels)\n",
    "test_dataset = TensorDataset(test_inputs, test_masks, test_labels)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, num_workers=4)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, num_workers=4)\n",
    "\n",
    "# Initialize model\n",
    "print(\"Initializing model...\")\n",
    "model = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=2)\n",
    "model.to(device)\n",
    "\n",
    "# Optimizer and Scheduler\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "num_training_steps = len(train_loader) * 3  # Assuming 3 epochs\n",
    "lr_scheduler = get_scheduler(\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps)\n",
    "\n",
    "# Training loop\n",
    "epochs = 3\n",
    "print(\"Starting training...\")\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for step, batch in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        input_ids = batch[0].to(device)\n",
    "        attention_mask = batch[1].to(device)\n",
    "        labels = batch[2].to(device)\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        if step % 100 == 0:\n",
    "            print(f\"Epoch {epoch + 1}, Step {step}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "    print(f\"Epoch {epoch + 1} completed. Average Loss: {total_loss / len(train_loader):.4f}\")\n",
    "\n",
    "# Evaluation\n",
    "print(\"Evaluating model...\")\n",
    "model.eval()\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        input_ids = batch[0].to(device)\n",
    "        attention_mask = batch[1].to(device)\n",
    "        labels = batch[2].to(device)\n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits\n",
    "        preds = torch.argmax(logits, dim=-1)\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "accuracy = accuracy_score(all_labels, all_preds)\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4uYY2XCCAthe"
   },
   "source": [
    "#Class 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z5iLML98CrpX"
   },
   "outputs": [],
   "source": [
    "from transformers import PreTrainedModel, PretrainedConfig, DistilBertForSequenceClassification, DistilBertTokenizer\n",
    "from transformers.utils import logging\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from torch.optim import AdamW\n",
    "import torch\n",
    "import pandas as pd\n",
    "from huggingface_hub import login\n",
    "import re\n",
    "\n",
    "\n",
    "from huggingface_hub import push_to_hub_keras\n",
    "from huggingface_hub import notebook_login\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Vt7KC03FCt5R"
   },
   "outputs": [],
   "source": [
    "logging.set_verbosity_error()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-F99lMulAvYN"
   },
   "outputs": [],
   "source": [
    "class NewsClassifierConfig(PretrainedConfig):\n",
    "    model_type = \"news_classifier\"\n",
    "\n",
    "    def __init__(self, num_labels: int = 2, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.num_labels = num_labels\n",
    "\n",
    "class NewsClassifier(PreTrainedModel):\n",
    "    config_class = NewsClassifierConfig\n",
    "\n",
    "    def __init__(self, config: NewsClassifierConfig):\n",
    "        super().__init__(config)\n",
    "        self.model = DistilBertForSequenceClassification.from_pretrained(\n",
    "            \"distilbert-base-uncased\", num_labels=config.num_labels\n",
    "        )\n",
    "        self.tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "    def clean_text(self, text: str) -> str:\n",
    "        \"\"\"Clean and preprocess text.\"\"\"\n",
    "        text = str(text)\n",
    "        contractions = {\n",
    "            \"n't\": \" not\",\n",
    "            \"'s\": \" is\",\n",
    "            \"'ll\": \" will\",\n",
    "            \"'ve\": \" have\"\n",
    "        }\n",
    "        for contraction, expansion in contractions.items():\n",
    "            text = text.replace(contraction, expansion)\n",
    "        text = re.sub(r'\\$\\\\d+\\.?\\\\d*\\s*(million|billion|trillion)?', r'$ \\1', text, flags=re.IGNORECASE)\n",
    "        text = re.sub(r'http\\\\S+', '', text)\n",
    "        text = re.sub(r'-', ' ', text)\n",
    "        text = text.lower()\n",
    "        text = ' '.join(text.split())\n",
    "        return text\n",
    "\n",
    "    def tokenize(self, texts, max_len=128):\n",
    "        cleaned_texts = [self.clean_text(text) for text in texts]\n",
    "        return self.tokenizer(\n",
    "            cleaned_texts,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=max_len,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, labels=None):\n",
    "        return self.model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "\n",
    "\n",
    "    def save_pretrained(self, save_directory, **kwargs):\n",
    "        super().save_pretrained(save_directory, **kwargs)\n",
    "        self.tokenizer.save_pretrained(save_directory)\n",
    "\n",
    "\n",
    "    @classmethod\n",
    "    def from_pretrained(cls, pretrained_model_name_or_path, *model_args, **kwargs):\n",
    "        model = super().from_pretrained(pretrained_model_name_or_path, *model_args, **kwargs)\n",
    "        model.tokenizer = DistilBertTokenizer.from_pretrained(pretrained_model_name_or_path)\n",
    "        return model\n",
    "\n",
    "\n",
    "class NewsDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_len=128):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            max_length=self.max_len,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        return {\n",
    "            \"input_ids\": encoding[\"input_ids\"].squeeze(),\n",
    "            \"attention_mask\": encoding[\"attention_mask\"].squeeze(),\n",
    "            \"labels\": torch.tensor(label, dtype=torch.long)\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rvrfeQJtCyMM",
    "outputId": "f0b43fb0-cb18-48a1-da94-87214ae19722"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.5149\n",
      "Epoch 2, Loss: 0.2561\n",
      "Epoch 3, Loss: 0.1015\n",
      "Test Accuracy: 0.8483\n"
     ]
    }
   ],
   "source": [
    "# Initialize model and tokenizer\n",
    "config = NewsClassifierConfig(num_labels=2)\n",
    "model = NewsClassifier(config)\n",
    "\n",
    "# Prepare datasets and dataloaders\n",
    "train_dataset = NewsDataset(X_train, y_train, model.tokenizer)\n",
    "test_dataset = NewsDataset(X_test, y_test, model.tokenizer)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, num_workers=2)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, num_workers=2)\n",
    "\n",
    "# Training setup\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "# Training loop\n",
    "epochs = 3\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch + 1}, Loss: {total_loss / len(train_loader):.4f}\")\n",
    "\n",
    "# Evaluation\n",
    "model.eval()\n",
    "all_preds, all_labels = [], []\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits\n",
    "        preds = torch.argmax(logits, dim=-1)\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "accuracy = accuracy_score(all_labels, all_preds)\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w0f9dRRI66SK"
   },
   "source": [
    "##sample test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9YZmEPcZJ5hZ"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import DistilBertForSequenceClassification, DistilBertTokenizer\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_7kTh8yiGOzV"
   },
   "outputs": [],
   "source": [
    "df_test = pd.read_csv('/content/drive/MyDrive/5190_project/test_data_random_subset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YigioSdQ69zJ"
   },
   "outputs": [],
   "source": [
    "# Load the model and tokenizer\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = NewsClassifier.from_pretrained(\"/content/drive/MyDrive/5190_project/news_transformer_model\")\n",
    "model.to(device)\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "O-EOuqV-KzCK",
    "outputId": "2245e2a2-24f1-4614-8393-e170ec596c25"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "print(model.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FvGgX-JHGJsV"
   },
   "outputs": [],
   "source": [
    "# Define a dataset for handling tokenization\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts, labels=None, tokenizer=None, max_len=128):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            None,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            padding='max_length',\n",
    "            return_token_type_ids=False,\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        input_ids = inputs['input_ids'].squeeze()\n",
    "        attention_mask = inputs['attention_mask'].squeeze()\n",
    "\n",
    "        if self.labels is not None:\n",
    "            label = self.labels[idx]\n",
    "            return {\n",
    "                'input_ids': input_ids,\n",
    "                'attention_mask': attention_mask,\n",
    "                'labels': torch.tensor(label, dtype=torch.long)\n",
    "            }\n",
    "        return {\n",
    "            'input_ids': input_ids,\n",
    "            'attention_mask': attention_mask\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "opUeqwpmGVpi"
   },
   "outputs": [],
   "source": [
    "# Prepare the test data loader\n",
    "test_dataset = TextDataset(df_test['title'].tolist(), df_test['labels'].tolist(), tokenizer, max_len=128)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LG65jrvDIo22"
   },
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "model.eval()\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        preds = torch.argmax(outputs.logits, dim=1)\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "j-h3j-h8JxBH",
    "outputId": "1b97e21e-0554-42df-c6d5-b9f94e59cca5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         NBC       0.75      0.60      0.67        10\n",
      "     FoxNews       0.67      0.80      0.73        10\n",
      "\n",
      "    accuracy                           0.70        20\n",
      "   macro avg       0.71      0.70      0.70        20\n",
      "weighted avg       0.71      0.70      0.70        20\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(all_labels, all_preds, target_names=['NBC', 'FoxNews']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TsziYf1YR_sU"
   },
   "source": [
    "##Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 207
    },
    "id": "vIaUZwJmR9Us",
    "outputId": "fa5ca416-c4bc-4f39-ccb3-233daac3dc61"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-376f1faeff43>\u001b[0m in \u001b[0;36m<cell line: 11>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# DataLoader setup with new batch size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mtrain_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_workers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0mtest_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_workers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_dataset' is not defined"
     ]
    }
   ],
   "source": [
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "\n",
    "# Hyperparameters\n",
    "learning_rate = 2e-5\n",
    "batch_size = 32\n",
    "epochs = 4\n",
    "max_grad_norm = 1.0\n",
    "\n",
    "# DataLoader setup with new batch size\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n",
    "\n",
    "# Optimizer setup\n",
    "optimizer = AdamW(model.parameters(), lr=learning_rate, eps=1e-8)\n",
    "\n",
    "# Total number of training steps\n",
    "num_training_steps = epochs * len(train_loader)\n",
    "\n",
    "# Scheduler and gradient clipping\n",
    "lr_scheduler = get_linear_schedule_with_warmup(optimizer,\n",
    "                                               num_warmup_steps=0,\n",
    "                                               num_training_steps=num_training_steps)\n",
    "\n",
    "# Adjusted training loop with gradient clipping\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for step, batch in enumerate(train_loader):\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "\n",
    "        # Gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        if step % 100 == 0:\n",
    "            print(f\"Epoch {epoch + 1}, Step {step}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "    print(f\"Epoch {epoch + 1} completed. Average Loss: {total_loss / len(train_loader):.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fzutoObV1W4s"
   },
   "source": [
    "# Class 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vv84YNWB1vzy"
   },
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    PreTrainedModel, PretrainedConfig, BertForSequenceClassification, BertTokenizer,\n",
    "    RobertaForSequenceClassification, RobertaTokenizer\n",
    ")\n",
    "from transformers.utils import logging\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from torch.optim import AdamW\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import re\n",
    "from huggingface_hub import login\n",
    "\n",
    "\n",
    "from sklearn.model_selection import KFold, ParameterGrid\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from transformers import AdamW, get_scheduler\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "logging.set_verbosity_error()\n",
    "from transformers import AutoModel, AutoTokenizer, AutoConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4GvhgrKK1ZLe"
   },
   "outputs": [],
   "source": [
    "class NewsClassifierConfig(PretrainedConfig):\n",
    "    model_type = \"news_classifier\"\n",
    "\n",
    "    def __init__(self, num_labels: int = 2, model_type: str = \"bert\", **kwargs):\n",
    "        \"\"\"\n",
    "        model_type: 'bert' or 'roberta'\n",
    "        \"\"\"\n",
    "        super().__init__(**kwargs)\n",
    "        self.num_labels = num_labels\n",
    "        self.model_type = model_type\n",
    "\n",
    "class NewsClassifier(PreTrainedModel):\n",
    "    config_class = NewsClassifierConfig\n",
    "\n",
    "    def __init__(self, config: NewsClassifierConfig):\n",
    "        super().__init__(config)\n",
    "\n",
    "        # Choose between BERT and RoBERTa based on config\n",
    "        if config.model_type == \"bert\":\n",
    "            self.model = BertForSequenceClassification.from_pretrained(\n",
    "                \"bert-base-uncased\", num_labels=config.num_labels\n",
    "            )\n",
    "            self.tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "        elif config.model_type == \"roberta\":\n",
    "            self.model = RobertaForSequenceClassification.from_pretrained(\n",
    "                \"roberta-base\", num_labels=config.num_labels\n",
    "            )\n",
    "            self.tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")\n",
    "        else:\n",
    "            raise ValueError(\"Invalid model_type. Choose 'bert' or 'roberta'.\")\n",
    "\n",
    "    def clean_text(self, text: str) -> str:\n",
    "        \"\"\"Clean and preprocess text.\"\"\"\n",
    "        text = str(text)\n",
    "        contractions = {\n",
    "            \"n't\": \" not\",\n",
    "            \"'s\": \" is\",\n",
    "            \"'ll\": \" will\",\n",
    "            \"'ve\": \" have\"\n",
    "        }\n",
    "        for contraction, expansion in contractions.items():\n",
    "            text = text.replace(contraction, expansion)\n",
    "        text = re.sub(r'\\$\\\\d+\\.?\\\\d*\\s*(million|billion|trillion)?', r'$ \\1', text, flags=re.IGNORECASE)\n",
    "        text = re.sub(r'http\\\\S+', '', text)\n",
    "        text = re.sub(r'-', ' ', text)\n",
    "        text = text.lower()\n",
    "        text = ' '.join(text.split())\n",
    "        return text\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, labels=None):\n",
    "        return self.model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "\n",
    "    def save_pretrained(self, save_directory, **kwargs):\n",
    "        super().save_pretrained(save_directory, **kwargs)\n",
    "        self.tokenizer.save_pretrained(save_directory)\n",
    "\n",
    "    @classmethod\n",
    "    def from_pretrained(cls, pretrained_model_name_or_path, *model_args, **kwargs):\n",
    "        config = NewsClassifierConfig.from_pretrained(pretrained_model_name_or_path, **kwargs)\n",
    "        model = super(NewsClassifier, cls).from_pretrained(pretrained_model_name_or_path, config=config, *model_args, **kwargs)\n",
    "\n",
    "        # Load tokenizer separately\n",
    "        tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path)\n",
    "        model.tokenizer = tokenizer\n",
    "        return model\n",
    "\n",
    "\n",
    "class NewsDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_len=128):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            max_length=self.max_len,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        return {\n",
    "            \"input_ids\": encoding[\"input_ids\"].squeeze(),\n",
    "            \"attention_mask\": encoding[\"attention_mask\"].squeeze(),\n",
    "            \"labels\": torch.tensor(label, dtype=torch.long)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FVnXAz-9ZmKJ"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoConfig, AutoModel\n",
    "# Register the custom classes\n",
    "AutoConfig.register(\"news_classifier\", NewsClassifierConfig)\n",
    "AutoModel.register(NewsClassifierConfig, NewsClassifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Eo0Gusu8185Z"
   },
   "outputs": [],
   "source": [
    "# Initialize Hugging Face login\n",
    "HF_TOKEN = \"REPLACE WITH TOKEN\"\n",
    "login(HF_TOKEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pJ4FeJwj2JYm"
   },
   "outputs": [],
   "source": [
    "# Load data\n",
    "news_data = pd.read_csv(\"/content/drive/My Drive/5190_project/news_data.csv\")\n",
    "X = news_data['title'].values\n",
    "y = news_data['labels'].values\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Te1FtQXQLeyp"
   },
   "source": [
    "## Model 1 BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 376,
     "referenced_widgets": [
      "39a268f7e1504700b23d4293693538f9",
      "e6e121501feb4372ad46b5fd762f404e",
      "618d373ba8cf46b2a682aa108f901814",
      "89e4f7fc6a8440e2ab0f1e95ddc5ff1e",
      "75f694e780304574b46768e94a199ee8",
      "1cc7fa5dc28346228c044400b1c7b894",
      "52ed54da37634f17beb0b5b195df6035",
      "df9ff47d145f4400a6067a9d87732c09",
      "19cb2ea91a8f441da73a07cbbd5323fc",
      "1a7c4ae70feb4ff68f526cfb98e6f290",
      "27d0a521a6f14fe19b843cee959d8b51",
      "509b76913ec8492cb6ab5af7e43e9084",
      "bdc3b7a40ed54ce2b9c328300771dd55",
      "42f5968d831142f3aa267b40c1bb0901",
      "2c49606f119c4a26913d563d98b2b784",
      "1f30eacdcaf7406c90a78f1884ec590b",
      "38e4c88e60a6458f9c9092d6f21f3a39",
      "07a3791c66044a57b213dd0d6bf2b914",
      "ff3144c370704166b11d8f71a1e68ed0",
      "f78ed2b6718c4b02a72e51010322996a",
      "0f8c38d3f614462eabf27cc72984de0c",
      "5236fdd367e64f49b4720f9b5fb47462",
      "00002e1ca81f43b184d6284b40b32e0c",
      "3a0052149b8b4bffa9cb6a16b6d25ba9",
      "35c6d52a887a4e8b8d5d4edde04e870f",
      "2ab2d82f1ae3430bb4d70ba75f15c46b",
      "f0c43072a41d49ec8d050d2bad0a1fc8",
      "3812cdc71e0a40159f19bd683ea07881",
      "631ee04eb50f4ec8b924e8162d754e8a",
      "b0a994a6d0dd49d6a9b52f3e02089273",
      "945271e00d8b4f29bed9bb7678632861",
      "ce51534704664bceaacf758536580cb9",
      "e5b700c802394ed58b133f4feacb5e48",
      "d0096de8d66b45bea2d206815a3a1b87",
      "6e6eaab4c140499d8c0719e3a1de4623",
      "d1bc3cfc9be94234b8a1fcbf05fc11f2",
      "62f73fb7084644948fa3d75b6f99fcd3",
      "a2e39f508fb04c48baf91c13335724ae",
      "82a0a78ed433477fbc023432abefe265",
      "994c72760194418e893b435a3dcd464d",
      "5b855f63e5a344cb95dacf61a80eb727",
      "c1ee6c999c34448383c8ddb1da653582",
      "7dcdf60f895545cfba40caa248a24b89",
      "19c7d78aa6a84c708b41c2d25f28393d",
      "8b51603e638a410ca77311174a8d03b1",
      "af7d6a66469449579b7f254844a46b1e",
      "b8252d4f84994a638f6f9339074eda9f",
      "0d44d85dae724557be6e9bde5e730ad2",
      "64950d1bce9945a0b13604ca869fc464",
      "a6c15a3fd436428097227f9107632148",
      "20f8695949c74932b6fc1c9cb1bf8dfc",
      "9fd703b311e049aeb00e367ea7291888",
      "f5428f1d905d49c89bec0be98f2ef308",
      "8014cf224633409bb845746419b51736",
      "7ac3421f45c04e26ba7b26dad98578da"
     ]
    },
    "id": "7-Hi4jY91zzw",
    "outputId": "d691ec21-7e48-433b-dced-0136b0506e96"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39a268f7e1504700b23d4293693538f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "509b76913ec8492cb6ab5af7e43e9084",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00002e1ca81f43b184d6284b40b32e0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0096de8d66b45bea2d206815a3a1b87",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b51603e638a410ca77311174a8d03b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.5309\n",
      "Epoch 2, Loss: 0.2537\n",
      "Epoch 3, Loss: 0.1000\n",
      "Test Accuracy: 0.8647\n"
     ]
    }
   ],
   "source": [
    "# model_type=\"bert\"\n",
    "config = NewsClassifierConfig(num_labels=2, model_type=\"bert\")\n",
    "model = NewsClassifier(config)\n",
    "\n",
    "# Prepare datasets and dataloaders\n",
    "train_dataset = NewsDataset(X_train, y_train, model.tokenizer)\n",
    "test_dataset = NewsDataset(X_test, y_test, model.tokenizer)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, num_workers=2)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, num_workers=2)\n",
    "\n",
    "# Training setup\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "optimizer = AdamW(model.parameters(), lr=3e-5)\n",
    "\n",
    "# Training loop\n",
    "epochs = 3\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch + 1}, Loss: {total_loss / len(train_loader):.4f}\")\n",
    "\n",
    "# Evaluation\n",
    "model.eval()\n",
    "all_preds, all_labels = [], []\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits\n",
    "        preds = torch.argmax(logits, dim=-1)\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "accuracy = accuracy_score(all_labels, all_preds)\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i4UOe5ML8EHG"
   },
   "source": [
    "## Model 2: RoBERTa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lwTbKTcP2MGZ",
    "outputId": "5c1f46f9-9f08-4606-ec8a-16a2dfb6fb50"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.5741\n",
      "Epoch 2, Loss: 0.3464\n",
      "Epoch 3, Loss: 0.1999\n",
      "Test Accuracy: 0.8739\n"
     ]
    }
   ],
   "source": [
    "# model_type=\"roberta\"\n",
    "config = NewsClassifierConfig(num_labels=2, model_type=\"roberta\")\n",
    "model = NewsClassifier(config)\n",
    "\n",
    "# Prepare datasets and dataloaders\n",
    "train_dataset = NewsDataset(X_train, y_train, model.tokenizer)\n",
    "test_dataset = NewsDataset(X_test, y_test, model.tokenizer)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, num_workers=2)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, num_workers=2)\n",
    "\n",
    "# Training setup\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "optimizer = AdamW(model.parameters(), lr=3e-5)\n",
    "\n",
    "# Training loop\n",
    "epochs = 3\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch + 1}, Loss: {total_loss / len(train_loader):.4f}\")\n",
    "\n",
    "# Evaluation\n",
    "model.eval()\n",
    "all_preds, all_labels = [], []\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits\n",
    "        preds = torch.argmax(logits, dim=-1)\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "accuracy = accuracy_score(all_labels, all_preds)\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hHlIZEbjVYQx"
   },
   "outputs": [],
   "source": [
    "new_test = pd.read_csv(\"/content/drive/MyDrive/5190_project/test_data_random_subset.csv\")\n",
    "X_test_new = new_test['title'].values\n",
    "y_test_new = new_test['labels'].values\n",
    "test_dataset_new = NewsDataset(X_test_new, y_test_new, model.tokenizer)\n",
    "test_loader_new = DataLoader(test_dataset_new, batch_size=16, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gDxiwLfRVbMy",
    "outputId": "3a192398-bf3c-40eb-c92c-92e10886df12"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.8500\n"
     ]
    }
   ],
   "source": [
    "# Evaluation\n",
    "model.eval()\n",
    "all_preds, all_labels = [], []\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader_new:\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits\n",
    "        preds = torch.argmax(logits, dim=-1)\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "accuracy = accuracy_score(all_labels, all_preds)\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gcFLtCHvLVRA"
   },
   "source": [
    "## 5-Fold Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ASgabLW-7MAf"
   },
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    \"learning_rate\": [2e-5, 3e-5, 5e-5],\n",
    "    \"batch_size\": [8, 16],\n",
    "    \"num_epochs\": [3, 4]\n",
    "}\n",
    "\n",
    "def train_and_evaluate(train_idx, val_idx, dataset, model, device, lr, batch_size, num_epochs):\n",
    "    # Create subsets\n",
    "    train_subset = Subset(dataset, train_idx)\n",
    "    val_subset = Subset(dataset, val_idx)\n",
    "\n",
    "    # Prepare dataloaders\n",
    "    train_loader = DataLoader(train_subset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_subset, batch_size=batch_size)\n",
    "\n",
    "    # Optimizer and learning rate scheduler\n",
    "    optimizer = AdamW(model.parameters(), lr=lr)\n",
    "    num_training_steps = len(train_loader) * num_epochs\n",
    "    scheduler = get_scheduler(\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps)\n",
    "\n",
    "    # Training loop\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        for batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}, Loss: {total_loss / len(train_loader):.4f}\")\n",
    "\n",
    "    # Evaluation\n",
    "    model.eval()\n",
    "    val_preds, val_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            preds = torch.argmax(outputs.logits, dim=-1)\n",
    "            val_preds.extend(preds.cpu().numpy())\n",
    "            val_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    accuracy = accuracy_score(val_labels, val_preds)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bzn9eNqM7piO"
   },
   "outputs": [],
   "source": [
    "# Hyperparameter Tuning and Cross-Validation\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "best_accuracy = 0\n",
    "best_params = None\n",
    "best_model = None\n",
    "\n",
    "# Create the dataset\n",
    "dataset = NewsDataset(X, y, model.tokenizer)\n",
    "\n",
    "for params in ParameterGrid(param_grid):\n",
    "    lr = params[\"learning_rate\"]\n",
    "    batch_size = params[\"batch_size\"]\n",
    "    num_epochs = params[\"num_epochs\"]\n",
    "\n",
    "    print(f\"\\nTesting Hyperparameters: LR={lr}, Batch Size={batch_size}, Epochs={num_epochs}\")\n",
    "    fold_accuracies = []\n",
    "\n",
    "    for fold, (train_idx, val_idx) in enumerate(kf.split(X)):\n",
    "        print(f\"  Fold {fold + 1}...\")\n",
    "        # Reinitialize the model for each fold\n",
    "        config = NewsClassifierConfig(num_labels=2, model_type=\"roberta\")\n",
    "        model = NewsClassifier(config)\n",
    "\n",
    "        # Train and evaluate\n",
    "        accuracy = train_and_evaluate(train_idx, val_idx, dataset, model, device, lr, batch_size, num_epochs)\n",
    "        fold_accuracies.append(accuracy)\n",
    "        print(f\"    Fold {fold + 1} Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "    mean_accuracy = np.mean(fold_accuracies)\n",
    "    print(f\"Mean Accuracy for LR={lr}, Batch Size={batch_size}, Epochs={num_epochs}: {mean_accuracy:.4f}\")\n",
    "\n",
    "    # Update best model and hyperparameters\n",
    "    if mean_accuracy > best_accuracy:\n",
    "        best_accuracy = mean_accuracy\n",
    "        best_params = params\n",
    "        best_model = model  # Save the best model\n",
    "\n",
    "print(\"\\nBest Hyperparameters:\", best_params)\n",
    "print(\"Best Cross-Validation Accuracy:\", best_accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lo4pE_rsLZqO"
   },
   "source": [
    "## Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KkveeTG17tpv",
    "outputId": "4d11e788-6063-4987-daff-6b1c77364f74"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and tokenizer saved locally at: /content/drive/MyDrive/5190_project/news_classifier_roberta\n"
     ]
    }
   ],
   "source": [
    "# Save the model\n",
    "save_directory = \"/content/drive/MyDrive/5190_project/news_classifier_roberta\"\n",
    "model.save_pretrained(save_directory)\n",
    "model.tokenizer.save_pretrained(save_directory)\n",
    "print(f\"Model and tokenizer saved locally at: {save_directory}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 117,
     "referenced_widgets": [
      "0236882960ce4bf5ae784738a7d498e7",
      "ffb7e0f709c748e480b73a937f70430c",
      "b4105e6ad07b4544ab4da61685583030",
      "1b648df3eeb94ea7a3f9d8f13ccd70c4",
      "e773c5f23d364db7ac9be6947ba004dc",
      "d4f0c749f1404c5aba99f4ee139c0bdb",
      "e91a73eec4b9419298dabf618bd7daca",
      "42d934de315541f480586cf4df7c039a",
      "3c92a8bd03384141b7d6f93cbca8297f",
      "7fdf3ed5de734de1b9e22619287cd0ce",
      "5407991176b5474e9365e5dbca57dd3a",
      "7029770049d84fe79d90af7fe7c6c8d1",
      "a2e603dca6d34bfd8efa55c1b29040d3",
      "cf665d3b8c29418983b67307c104fcba",
      "3d9415c8c88f4255a99f1787874949bd",
      "fc4df937d2284cb7a08542bd7d7258e7",
      "0b0baf878df44493a92a86b9551b91eb",
      "984632229af3441cb7960ffc09761c49",
      "8a5835c7d1834ae8a99822b055b0ee4d",
      "837e280d1b8c4301b272e0fc78f7c394",
      "18a2c539286641b693e682d75557fa15",
      "c9cbead95972481d99838ed743c325f6"
     ]
    },
    "id": "hYEY4EJo2CjM",
    "outputId": "c111d2d3-1907-4db0-8b54-ea1531366a46"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pushing model to Hugging Face Hub (CIS5190GoGo/NewsClassifierConfig)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0236882960ce4bf5ae784738a7d498e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/2.63k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7029770049d84fe79d90af7fe7c6c8d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/499M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model successfully pushed to https://huggingface.co/CIS5190GoGo/NewsClassifierConfig\n"
     ]
    }
   ],
   "source": [
    "# Push to Hugging Face Hub\n",
    "REPO_NAME = \"CIS5190GoGo/NewsClassifierConfig\"\n",
    "print(f\"Pushing model to Hugging Face Hub ({REPO_NAME})...\")\n",
    "model.push_to_hub(REPO_NAME)\n",
    "print(f\"Model successfully pushed to https://huggingface.co/{REPO_NAME}\")\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
